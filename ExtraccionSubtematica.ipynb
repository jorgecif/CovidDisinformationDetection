{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFM-ExtraccionTemaInformacionCOVID",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZSU8mIKacTfcu2nsc5i2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgecif/CovidDisinformationDetection/blob/main/ExtraccSubtemaInfoCOVID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f_yS6Lc0sZh"
      },
      "source": [
        "# **Extración de temáticas de información relacionada con el COVID19** - Comparación de modelos de aprendizaje automático y aprendizaje profundo\n",
        "\n",
        "\n",
        "> Por: Jorge Orlando Cifuentes Cifuentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfdtHYHA1wTE"
      },
      "source": [
        "### Librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqMfnQPO1jDH",
        "outputId": "1630392b-3fac-41b8-b4ea-56cfafbb93b2"
      },
      "source": [
        "# Generales\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "np.random.seed(88)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Gráficas\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukPIkts91kxA"
      },
      "source": [
        "### Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "wloEBaLz1jA0",
        "outputId": "0c476f37-b8fc-43e3-84ee-3cd2d26b4c19"
      },
      "source": [
        "# Carga de datos\n",
        "url_datos = 'https://github.com/jorgecif/CovidMisinformationDetection/blob/main/data/CovidHeadlinesDataset.xlsx?raw=true' \n",
        "\n",
        "datos = pd.read_excel(url_datos)\n",
        "\n",
        "print(\"Se han cargado \", datos.shape[0], \" noticias\")\n",
        "\n",
        "# Muestra de los datos\n",
        "print(len(datos))\n",
        "datos.head()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Se han cargado  21721  noticias\n",
            "21721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>country</th>\n",
              "      <th>lang</th>\n",
              "      <th>label</th>\n",
              "      <th>URL</th>\n",
              "      <th>date</th>\n",
              "      <th>verificado_por</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Britain’s Death Toll from the Coronavirus Riva...</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>English</td>\n",
              "      <td>True</td>\n",
              "      <td>https://www.snopes.com/ap/2020/05/05/britains-...</td>\n",
              "      <td>2020-02-02 00:00:00</td>\n",
              "      <td>snopes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Timeline Reset: CDC Confirms Weeks-Earlier Cal...</td>\n",
              "      <td>United States</td>\n",
              "      <td>English</td>\n",
              "      <td>True</td>\n",
              "      <td>https://www.snopes.com/ap/2020/04/22/timeline-...</td>\n",
              "      <td>2020-06-03 00:00:00</td>\n",
              "      <td>snopes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dentists, Hair Salons, Beaches: Lockdowns Ease...</td>\n",
              "      <td>United States</td>\n",
              "      <td>English</td>\n",
              "      <td>True</td>\n",
              "      <td>https://www.snopes.com/ap/2020/04/20/dentists-...</td>\n",
              "      <td>2020-08-03 00:00:00</td>\n",
              "      <td>snopes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33 Million Have Sought US Unemployment Aid Sin...</td>\n",
              "      <td>United States</td>\n",
              "      <td>English</td>\n",
              "      <td>True</td>\n",
              "      <td>https://www.snopes.com/ap/2020/05/07/33-millio...</td>\n",
              "      <td>2020-09-03 00:00:00</td>\n",
              "      <td>snopes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>United States ‘Wasted’ Months Before Preparing...</td>\n",
              "      <td>United States</td>\n",
              "      <td>English</td>\n",
              "      <td>True</td>\n",
              "      <td>https://www.snopes.com/ap/2020/04/05/u-s-waste...</td>\n",
              "      <td>2020-03-04 00:00:00</td>\n",
              "      <td>snopes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  ... verificado_por\n",
              "0  Britain’s Death Toll from the Coronavirus Riva...  ...         snopes\n",
              "1  Timeline Reset: CDC Confirms Weeks-Earlier Cal...  ...         snopes\n",
              "2  Dentists, Hair Salons, Beaches: Lockdowns Ease...  ...         snopes\n",
              "3  33 Million Have Sought US Unemployment Aid Sin...  ...         snopes\n",
              "4  United States ‘Wasted’ Months Before Preparing...  ...         snopes\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5m2exS3Av4W"
      },
      "source": [
        "### Creación de conjuntos de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNkehkPoAyQV",
        "outputId": "09111262-2f76-4777-f8dd-9f5e358062e3"
      },
      "source": [
        "# Reservo unos datos para pruebas finales (datos que no se tocan)\n",
        "\n",
        "datos_trabajo,datos_reserva = train_test_split(datos, test_size=0.1, random_state=88 )\n",
        "datos_trabajo=datos_trabajo.reset_index()\n",
        "datos_trabajo=datos_trabajo.drop([\"index\"], axis=1)\n",
        "\n",
        "datos_reserva=datos_reserva.reset_index()\n",
        "datos_reserva=datos_reserva.drop([\"index\"], axis=1)\n",
        "\n",
        "print(\"Datos para trabajar: \", len(datos_trabajo))\n",
        "print(\"Datos reservados para pruebas finales \", len(datos_reserva))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datos para trabajar:  19548\n",
            "Datos reservados para pruebas finales  2173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custBOgQNICz"
      },
      "source": [
        "### Creació de lista de documentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEULdvDF6A2K"
      },
      "source": [
        "# Creo lista con titulares a procesar\n",
        "headlines=datos_trabajo[\"Text\"]\n",
        "docs_train=headlines.values.tolist()\n",
        "\n",
        "headlines=datos_reserva[\"Text\"]\n",
        "docs_test=headlines.values.tolist()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "7AdbzkA-571p",
        "outputId": "87b91942-be7a-4240-e03e-0f9171a754a4"
      },
      "source": [
        "# Convierto plural a singular\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
        "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
        "           'traditional', 'reference', 'colonizer','plotted']\n",
        "singles = [stemmer.stem(plural) for plural in original_words]\n",
        "\n",
        "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original word</th>\n",
              "      <th>stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>caresses</td>\n",
              "      <td>caress</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>flies</td>\n",
              "      <td>fli</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dies</td>\n",
              "      <td>die</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mules</td>\n",
              "      <td>mule</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>denied</td>\n",
              "      <td>deni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>died</td>\n",
              "      <td>die</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>agreed</td>\n",
              "      <td>agre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>owned</td>\n",
              "      <td>own</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>humbled</td>\n",
              "      <td>humbl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>sized</td>\n",
              "      <td>size</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>meeting</td>\n",
              "      <td>meet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>stating</td>\n",
              "      <td>state</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>siezing</td>\n",
              "      <td>siez</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>itemization</td>\n",
              "      <td>item</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>sensational</td>\n",
              "      <td>sensat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>traditional</td>\n",
              "      <td>tradit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>reference</td>\n",
              "      <td>refer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>colonizer</td>\n",
              "      <td>colon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>plotted</td>\n",
              "      <td>plot</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   original word stemmed\n",
              "0       caresses  caress\n",
              "1          flies     fli\n",
              "2           dies     die\n",
              "3          mules    mule\n",
              "4         denied    deni\n",
              "5           died     die\n",
              "6         agreed    agre\n",
              "7          owned     own\n",
              "8        humbled   humbl\n",
              "9          sized    size\n",
              "10       meeting    meet\n",
              "11       stating   state\n",
              "12       siezing    siez\n",
              "13   itemization    item\n",
              "14   sensational  sensat\n",
              "15   traditional  tradit\n",
              "16     reference   refer\n",
              "17     colonizer   colon\n",
              "18       plotted    plot"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SfJ0RnH6rlT"
      },
      "source": [
        "# Función de lematización\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "            \n",
        "    return result\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qupek0mV6-da",
        "outputId": "e722b068-750a-4229-a029-8c79a249ffa5"
      },
      "source": [
        "# Ejemplo\n",
        "\n",
        "document_num = 50\n",
        "doc_sample = 'This disk has failed many times. I would like to get it replaced.'\n",
        "\n",
        "print(\"Original document: \")\n",
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print(\"\\n\\nTokenized and lemmatized document: \")\n",
        "print(preprocess(doc_sample))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original document: \n",
            "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n",
            "\n",
            "\n",
            "Tokenized and lemmatized document: \n",
            "['disk', 'fail', 'time', 'like', 'replac']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5W7blyj7dYE"
      },
      "source": [
        "# Aplico función de lematización\n",
        "\n",
        "processed_docs = []\n",
        "\n",
        "for doc in docs_train:\n",
        "    processed_docs.append(preprocess(doc))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ0dMmv97gah",
        "outputId": "cab2e432-cde8-45e1-f1c7-3b2c7a296b3e"
      },
      "source": [
        "# Imprimo documentos procesados\n",
        "\n",
        "print(processed_docs[:2])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['woman', 'uttar', 'pradesh', 'throw', 'children', 'river', 'food', 'current', 'lockdown'], ['chart', 'show', 'current', 'case', 'turkey']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR1T_wpI9rmM"
      },
      "source": [
        "### Bolsa de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCB-nVSA9t1Y"
      },
      "source": [
        "# Creación del diccionario\n",
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy8qtyR398jf",
        "outputId": "df9ca3ce-8caa-4f48-88a1-74c7a0a22cea"
      },
      "source": [
        "# Creo diccionario\n",
        "\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 children\n",
            "1 current\n",
            "2 food\n",
            "3 lockdown\n",
            "4 pradesh\n",
            "5 river\n",
            "6 throw\n",
            "7 uttar\n",
            "8 woman\n",
            "9 case\n",
            "10 chart\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLwHbpR8-MJ0"
      },
      "source": [
        "# Remover palabras poco comunes\n",
        "\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taHKAuRf-Oeg"
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOm2UHQd-W2y",
        "outputId": "9e2cee26-0d5e-4ea6-ce22-f7a2e67d31b5"
      },
      "source": [
        "document_num = 4\n",
        "bow_doc_x = bow_corpus[document_num]\n",
        "\n",
        "for i in range(len(bow_doc_x)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
        "                                                     dictionary[bow_doc_x[i][0]], \n",
        "                                                     bow_doc_x[i][1]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 22 (\"outbreak\") appears 1 time.\n",
            "Word 23 (\"report\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylDwDT6Q-jCN"
      },
      "source": [
        "### Corro LDA usando la bolsa de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLljquqb-oh7"
      },
      "source": [
        "# Creación del modelo\n",
        "num_topics=5\n",
        "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
        "                                   num_topics = num_topics, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 10,\n",
        "                                   workers = 2,\n",
        "                                   alpha=[0.01]*num_topics,\n",
        "                                   eta=[0.01]*len(dictionary.keys()))\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCLAMoLStfRe"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlK-XLkcoD4g",
        "outputId": "ad9c4544-ae54-4d98-9a75-4c664dc92af4"
      },
      "source": [
        "# Imprimo los temas creados\r\n",
        "\r\n",
        "for idx, topic in lda_model.print_topics(-1):\r\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\r\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.017*\"china\" + 0.014*\"time\" + 0.013*\"australia\" + 0.011*\"video\" + 0.010*\"window\" + 0.010*\"set\" + 0.010*\"public\" + 0.007*\"virus\" + 0.006*\"play\" + 0.006*\"test\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.024*\"claim\" + 0.023*\"video\" + 0.020*\"show\" + 0.016*\"post\" + 0.013*\"facebook\" + 0.012*\"novel\" + 0.011*\"share\" + 0.011*\"lockdown\" + 0.011*\"cure\" + 0.010*\"india\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.018*\"virus\" + 0.015*\"china\" + 0.011*\"test\" + 0.010*\"hospit\" + 0.009*\"australia\" + 0.008*\"pictur\" + 0.007*\"patient\" + 0.007*\"travel\" + 0.006*\"mask\" + 0.006*\"symptom\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.018*\"death\" + 0.017*\"report\" + 0.017*\"countri\" + 0.012*\"vaccin\" + 0.012*\"number\" + 0.012*\"govern\" + 0.010*\"wednesday\" + 0.010*\"friday\" + 0.009*\"thursday\" + 0.009*\"monday\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.029*\"trump\" + 0.027*\"toilet\" + 0.026*\"paper\" + 0.023*\"work\" + 0.022*\"administr\" + 0.021*\"order\" + 0.020*\"prove\" + 0.020*\"requir\" + 0.019*\"resid\" + 0.019*\"rule\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGydAzf1-tb6",
        "outputId": "d70b8ece-d897-4a8e-bb58-9957025fc755"
      },
      "source": [
        "# Muestro los temas creados\n",
        "\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.017*\"china\" + 0.014*\"time\" + 0.013*\"australia\" + 0.011*\"video\" + 0.010*\"window\" + 0.010*\"set\" + 0.010*\"public\" + 0.007*\"virus\" + 0.006*\"play\" + 0.006*\"test\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.024*\"claim\" + 0.023*\"video\" + 0.020*\"show\" + 0.016*\"post\" + 0.013*\"facebook\" + 0.012*\"novel\" + 0.011*\"share\" + 0.011*\"lockdown\" + 0.011*\"cure\" + 0.010*\"india\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.018*\"virus\" + 0.015*\"china\" + 0.011*\"test\" + 0.010*\"hospit\" + 0.009*\"australia\" + 0.008*\"pictur\" + 0.007*\"patient\" + 0.007*\"travel\" + 0.006*\"mask\" + 0.006*\"symptom\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.018*\"death\" + 0.017*\"report\" + 0.017*\"countri\" + 0.012*\"vaccin\" + 0.012*\"number\" + 0.012*\"govern\" + 0.010*\"wednesday\" + 0.010*\"friday\" + 0.009*\"thursday\" + 0.009*\"monday\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.029*\"trump\" + 0.027*\"toilet\" + 0.026*\"paper\" + 0.023*\"work\" + 0.022*\"administr\" + 0.021*\"order\" + 0.020*\"prove\" + 0.020*\"requir\" + 0.019*\"resid\" + 0.019*\"rule\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1I7qiZFAaim"
      },
      "source": [
        "### Prueba de clasificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0crAwRXAe1p",
        "outputId": "a1ff1012-ffba-4aa3-a5df-6098c77af88b"
      },
      "source": [
        "num = 33\n",
        "unseen_document = docs_test[num]\n",
        "print(unseen_document)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Four Australian coronavirus cases confirmedSign up now: Special subscription offer – just $1 for first 28 daysFour South Australians are being tested for the deadly coronavirus. The outbreak started in China, where dozens of people have died and 36 million people are now in lockdown. Four cases have now been confirmed in Australia – one in Victoria and three in New South Wales.The state Health Department is managing four cases but Communicable Disease Control Branch director Louise Flood said the likelihood the patients are actually infected was “low”.Definitive results may not be available for several days.media_cameraPeople wearing masks due to coronavirus walk out of Customs after arriving in Australia from Shanghai. Picture: Chris Pavlich“This afternoon SA public health clinicians, department officials and the Minister for Health and Wellbeing Stephen Wade took part in a national teleconference with Commonwealth and state ministers and officials on the novel coronavirus response,” she said.MORE IN news‘Disabled just as entitled to sex worker’s services’premium_icon‘Disabled just as entitled to sex worker’s services’SA set for two budget blowouts of more than $1 billion – eachpremium_iconSA set for two budget blowouts of more than $1 billion – each\n",
            "\n",
            "“For precaution, four people in South Australia are being tested for coronavirus – but the likelihood they have the condition is low.“All required information on any SA links to confirmed interstate cases of coronavirus will be provided to SA Health to ensure immediate follow up and any further public health measures.“We continue to work closely with the Commonwealth Department of Health and Health Departments in all jurisdictions to maintain a close watch on the situation.”Video Player is loading.Play VideoPlayUP NEXTMuteLoaded: 0%Stream Type LIVERemaining Time -2:51Current Time 0:00/Duration 2:51Video AutoplayLog inPremium FeatureSwitching off the video autoplay is available only to The Advertiser subscribers. If you are a subscriber, please log in and access the video settings again.FullscreenPlayback Rate1xCaptionscaptions settings, opens captions settings dialogcaptions off, selectedThis is a modal window.Beginning of dialog window. Escape will cancel and close the window.TextColorTransparencyBackgroundColorTransparencyWindowColorTransparencyFont SizeText Edge StyleFont FamilyReset restore all settings to the default valuesDoneClose Modal DialogEnd of dialog window.PlayMuteCurrent Time 0:00/Duration 0:00Loaded: 0%Stream Type LIVESeek to live, currently playing liveLIVERemaining Time -0:00 Playback Rate1xFullscreen5The Advertiser2:51Coronavirus cases rise to four in Australia You just watchedYou're watchingvideo titleWatch againContinue watchingVideo paused due to user inactivityContinue watching?YesFour cases of the deadly coronavirus spreading from China in China have been confirmed in Australia with one case in Victoria and three in New South Wales. All four people inf...\n",
            "                                \n",
            "                            Chinese President Xi Jinping has warned of a “grave situation” as he said the killer coronavirus was “accelerating its speed”.Mr Xi held a special government meeting to discuss the spread of the virus that has killed 41 people and infected more than 1300 globally.China, the centre of the outbreak of the new coronavirus, will build a second dedicated hospital to treat patients.China cut off trains, planes and other links to China on Wednesday, as well as public transportation within the city, and has steadily expanded a lockdown to 16 surrounding cities with a combined population of more than 50 million.The outbreak has put a dampener on Chinese Lunar New Year celebrations around the world.Sign me upSigned UpView more Newsletters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anZF4fCCEQL1"
      },
      "source": [
        "# Preprocesamiento\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OifoYSKGEZTZ",
        "outputId": "12fd245b-eeb3-44f0-e4b0-5f545722a881"
      },
      "source": [
        "# Aplicación del modelo\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 15)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.9998413324356079\t Topic: 0.017*\"china\" + 0.014*\"time\" + 0.013*\"australia\" + 0.011*\"video\" + 0.010*\"window\" + 0.010*\"set\" + 0.010*\"public\" + 0.007*\"virus\" + 0.006*\"play\" + 0.006*\"test\" + 0.006*\"current\" + 0.006*\"advertis\" + 0.006*\"adelaid\" + 0.006*\"coupl\" + 0.006*\"chines\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3X060ISPRvH"
      },
      "source": [
        "### Guardo modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40mju-WsPUhl"
      },
      "source": [
        "## Guardo los modelos en archivos\n",
        "\n",
        "\n",
        "lda_model.save('model_LDA.h5') \n"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}
